{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braintriage challenge example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "Here we import important packages. Add any packages you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Modules to reload:\n",
      "all-except-skipped\n",
      "\n",
      "Modules to skip:\n",
      " models.omnipotent_resnet, models.combined_net. models.lstm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math as math\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pdb\n",
    "from torchvision import models\n",
    "\n",
    "## reload scripts before executing them\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## importing python files from another directory\n",
    "from dataset.create_slices import generate_slice_data\n",
    "from dataset.patient_dataframes import get_patient_train_val_dataframes\n",
    "from dataset.patient_dataset import PatientDataset\n",
    "from dataset.slice_dataframes import get_slice_train_val_dataframes\n",
    "from dataset.slice_dataset import SliceDataset\n",
    "from visualisation.slice_plotter import plot_slices\n",
    "\n",
    "from models.feature_vector_model import FeatureVectorModel\n",
    "\n",
    "## We need to make sure the Net class is only loaded once!\n",
    "## This is because the Net class calls the super(nn.Module) with itself as parameter\n",
    "## if we reload the class before every execution then the parameter will be another instantiantion and not \"itself\"\n",
    "from models.omnipotent_resnet import Net\n",
    "from models.combined_net import CombinedNet\n",
    "from models.lstm import LSTM\n",
    "%aimport - models.omnipotent_resnet, models.combined_net. models.lstm\n",
    "from train.train import Trainer\n",
    "%aimport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "This notebook serves as an example of how we could work in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '../../data/full'\n",
    "out_dir = '../../data_sliced'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb635b67de6a4865bc1153f1db41811f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Patients', max=500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092584b594fe446184ba3df349c11aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Patients', max=500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## generate slice data now accepts two parameters namely:\n",
    "## IN_DIR location of the raw ata\n",
    "## OUT_DIR preferred output location of the slices\n",
    "generate_slice_data(in_dir,out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patient_nr  slice_nr  class\n",
      "0         797         0      1\n",
      "1         797         1      1\n",
      "2         797         2      1\n",
      "3         797         3      1\n",
      "4         797         4      1\n",
      "Dataframe shape: (6336, 3)\n",
      "\n",
      "Number of unique patient numbers: 100\n",
      "Number of unique slice numbers:   32\n",
      "Number of unique class values:    2\n"
     ]
    }
   ],
   "source": [
    "label_df = pd.read_csv(os.path.join(DATA_DIR,\"labels_slices.csv\"), names = [\"patient_nr\", \"slice_nr\", \"class\"])\n",
    "label_df[\"class\"] = label_df[\"class\"].astype(\"int8\")\n",
    "patient_list = np.unique(label_df[\"patient_nr\"])\n",
    "print(label_df.head(), f\"Dataframe shape: {label_df.shape}\", sep=\"\\n\")\n",
    "print(f\"\\nNumber of unique patient numbers: {len(np.unique(label_df['patient_nr']))}\")\n",
    "print(f\"Number of unique slice numbers:   {len(np.unique(label_df['slice_nr']))}\")\n",
    "print(f\"Number of unique class values:    {len(np.unique(label_df['class']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_slices() missing 1 required positional argument: 'row_col_number'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-2ed64464a936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m797\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_slices() missing 1 required positional argument: 'row_col_number'"
     ]
    }
   ],
   "source": [
    "plot_slices(797, (0,31), DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset class that is compatible with the 'data_loader' from PyTorch. This allows us to feed our images to the network efficiently during training and validation. We read the labels for the different patients from a CSV file and convert this into a dictionary. Based on these labels we split the data in a training and validation set (you can choose a different ratio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global train/validation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Train on GPU or CPU\n",
    "TARGET_SLICES = (12,15)                                  # The slices we will train on for each patient\n",
    "N_FEATURES = 128                                         # The length of feature vectors that the CNN outputs/LSTM will use\n",
    "TRAIN_PERCENTAGE = 0.9                                   # Percentage of data that will be used for training\n",
    "\n",
    "model_dir = './models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  networks\n",
    "\n",
    "### Featurevector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "epochs = 1\n",
    "batch_size = 2\n",
    "n_features = 128\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "### Create model ###\n",
    "fc_net = FeatureVectorModel(n_features=n_features)\n",
    "\n",
    "### Loss and optimizer ###\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(fc_net.parameters(), lr=0.0001)\n",
    "\n",
    "### Create data generator (redefine for each network) ###\n",
    "train_df, val_df = get_slice_train_val_dataframes(label_df, train_percentage = TRAIN_PERCENTAGE)\n",
    "\n",
    "training_set = SliceDataset(train_df, TARGET_SLICES, DATA_DIR)\n",
    "validation_set = SliceDataset(val_df, TARGET_SLICES, DATA_DIR)\n",
    "\n",
    "training_iterator = data.DataLoader(training_set, batch_size=batch_size, shuffle = True)\n",
    "validation_iterator = data.DataLoader(validation_set, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate feature vector model\n",
    "First we instantiate the Trainer Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=fc_net, criterion=criterion, optimizer=optimizer, \n",
    "                   train_loader=training_iterator, val_loader=validation_iterator, n_epochs=epochs, model_dir = model_dir)\n",
    "trainer.train_and_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Parameters ###\n",
    "device = 0\n",
    "epochs = 1\n",
    "batch_size = 5\n",
    "in_channels = 3\n",
    "outsize = 1\n",
    "n_features = 128\n",
    "\n",
    "\n",
    "# Load Pre-Trained ResNet-50\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Change the Pre-Trained Model to our own Defined Model\n",
    "model = Net(model,\"resnet50\",n_features)\n",
    "\n",
    "\n",
    "# ### Loss and optimizer ###\n",
    "criterion = nn.BCEWithLogitsLoss()#nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "### Create data generator (redefine for each network) ###\n",
    "\n",
    "train_df, val_df = get_slice_train_val_dataframes(label_df, train_percentage = TRAIN_PERCENTAGE)\n",
    "\n",
    "training_set = SliceDataset(train_df, TARGET_SLICES, DATA_DIR)\n",
    "validation_set = SliceDataset(val_df, TARGET_SLICES, DATA_DIR)\n",
    "\n",
    "training_iterator = data.DataLoader(training_set, batch_size=batch_size, shuffle = True)\n",
    "validation_iterator = data.DataLoader(validation_set, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running resnet50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698e760e98034cfda75e9b78cdb273b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#epochs', max=1.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#train_batches', max=22.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#test_batches', max=3.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 / 001, training loss: 0.7019, validation loss: 0.7045, training accuracy: 0.482, validation accuracy: 0.467.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, criterion=criterion, optimizer=optimizer, \n",
    "                   train_loader=training_iterator, val_loader=validation_iterator, n_epochs=epochs, model_dir = model_dir)\n",
    "trainer.train_and_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate Resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Parameters ###\n",
    "device = 0\n",
    "epochs = 1\n",
    "batch_size = 5\n",
    "in_channels = 3\n",
    "outsize = 1\n",
    "n_features = 128\n",
    "\n",
    "\n",
    "\n",
    "# Load Pre-Trained ResNet-34\n",
    "model = models.resnet34(pretrained=True)\n",
    "\n",
    "# Change the Pre-Trained Model to our own Defined Model\n",
    "model = Net(model,\"resnet34\",n_features)\n",
    "\n",
    "\n",
    "# ### Loss and optimizer ###\n",
    "criterion = nn.BCEWithLogitsLoss()#nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "### Create data generator (redefine for each network) ###\n",
    "\n",
    "train_df, val_df = get_slice_train_val_dataframes(label_df, train_percentage = TRAIN_PERCENTAGE)\n",
    "\n",
    "training_set = SliceDataset(train_df, TARGET_SLICES, DATA_DIR)\n",
    "validation_set = SliceDataset(val_df, TARGET_SLICES, DATA_DIR)\n",
    "\n",
    "training_iterator = data.DataLoader(training_set, batch_size=batch_size, shuffle = True)\n",
    "validation_iterator = data.DataLoader(validation_set, batch_size=batch_size, shuffle = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, criterion=criterion, optimizer=optimizer, \n",
    "                   train_loader=training_iterator, val_loader=validation_iterator, n_epochs=epochs, model_dir = model_dir)\n",
    "trainer.train_and_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the simple cases are obvious now, lets see how it works with the combined model.\n",
    "\n",
    "-edit\n",
    "does not work, I think the problem is somewhere in the models will investigate more later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bf59c3b6a100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m### Create model ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlstm_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcombined_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCombinedNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# Turn off learning for cnn_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcombined_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_learning_cnn_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "## First train LSTM Model\n",
    "\n",
    "\n",
    "### Parameters ###\n",
    "epochs = 1\n",
    "batch_size = 5\n",
    "n_hidden = 64\n",
    "n_features = 128\n",
    "\n",
    "### Create model ###\n",
    "lstm_net = LSTM(n_features = n_features, n_hidden = n_hidden, n_layers = 1)\n",
    "combined_net = CombinedNet(cnn_net = model, lstm_net = lstm_net)\n",
    "# Turn off learning for cnn_net\n",
    "combined_net.set_learning_cnn_net(False)\n",
    "\n",
    "### Loss and optimizer ###\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(combined_net.parameters(), lr=0.0001)\n",
    "\n",
    "### Create data generator (redefine for each network) ###\n",
    "\n",
    "### Create data generator (redefine for each network) ###\n",
    "train_df, val_df, train_patients, val_patients = get_patient_train_val_dataframes(label_df, train_percentage = TRAIN_PERCENTAGE)\n",
    "\n",
    "training_set = PatientDataset(train_df, train_patients, TARGET_SLICES,DATA_DIR,DEVICE)\n",
    "validation_set = PatientDataset(val_df, val_patients, TARGET_SLICES,DATA_DIR,DEVICE)\n",
    "\n",
    "training_iterator = data.DataLoader(training_set, batch_size=batch_size, shuffle = True)\n",
    "validation_iterator = data.DataLoader(validation_set, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d0f8d4b20958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = Trainer(model=combined_net, criterion=criterion, optimizer=optimizer, \n\u001b[0m\u001b[1;32m      2\u001b[0m                    train_loader=training_iterator, val_loader=validation_iterator, n_epochs=epochs, model_dir = model_dir)\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_net' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=combined_net, criterion=criterion, optimizer=optimizer, \n",
    "                   train_loader=training_iterator, val_loader=validation_iterator, n_epochs=epochs, model_dir = model_dir)\n",
    "trainer.train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
