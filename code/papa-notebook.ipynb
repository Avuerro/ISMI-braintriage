{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Slice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data\n",
      "../../data/BrainTriage\n",
      "../../data/sliced-data\n",
      "../../data/sliced-data/train\n",
      "../../data/sliced-data/test\n",
      "../../data/data-split\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(\"..\",\"..\",\"data\")\n",
    "ORIGINAL_DATA_PATH = os.path.join(DATA_DIR,\"BrainTriage\")\n",
    "SLICED_DATA_DIR = os.path.join(DATA_DIR, \"sliced-data\")\n",
    "TRAIN_PATH = os.path.join(SLICED_DATA_DIR, \"train\")\n",
    "TEST_PATH = os.path.join(SLICED_DATA_DIR, \"test\")\n",
    "DATA_SPLIT_DIR = os.path.join(DATA_DIR, \"data-split\")\n",
    "print(DATA_DIR, ORIGINAL_DATA_PATH, SLICED_DATA_DIR, TRAIN_PATH, TEST_PATH, DATA_SPLIT_DIR, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TRAIN_PATH):\n",
    "    !python3 dataset/create_slices.py -d $ORIGINAL_DATA_PATH -o $SLICED_DATA_DIR --train\n",
    "if not os.path.exists(TEST_PATH):\n",
    "    !python3 dataset/create_slices.py -d $ORIGINAL_DATA_PATH -o $SLICED_DATA_DIR --test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\r\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATA_SPLIT_DIR):\n",
    "    !python3 dataset/create_data_split.py -k $K -d $TRAIN_PATH -ds $DATA_SPLIT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESNET_TYPE = \"resnet34\"\n",
    "SEED = 420\n",
    "LR = 0.0001\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "MODEL_DIR = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/fbergh/.netrc\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"train/train-resnet.py\", line 74, in <module>\n",
      "    train_df = pd.read_csv(os.path.join(args.ds_dir, \"train_df.csv\"), names=[\"patient_nr\", \"slice_nr\", \"class\"] ).sample(frac=1).reset_index(drop=True)\n",
      "  File \"/Users/fbergh/Library/Python/3.7/lib/python/site-packages/pandas/io/parsers.py\", line 676, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/fbergh/Library/Python/3.7/lib/python/site-packages/pandas/io/parsers.py\", line 448, in _read\n",
      "    parser = TextFileReader(fp_or_buf, **kwds)\n",
      "  File \"/Users/fbergh/Library/Python/3.7/lib/python/site-packages/pandas/io/parsers.py\", line 880, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"/Users/fbergh/Library/Python/3.7/lib/python/site-packages/pandas/io/parsers.py\", line 1114, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"/Users/fbergh/Library/Python/3.7/lib/python/site-packages/pandas/io/parsers.py\", line 1891, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 374, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas/_libs/parsers.pyx\", line 673, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: [Errno 2] File ../../data/test-split/train_df.csv does not exist: '../../data/test-split/train_df.csv'\n"
     ]
    }
   ],
   "source": [
    "!python3 train/train-resnet.py $RESNET_TYPE -s $SEED -lr $LR -e $EPOCHS -b $BATCH_SIZE -m \\\n",
    "                               $MODEL_DIR -d $TRAIN_PATH -ds $DATA_SPLIT_DIR -ts 0 32 --tuple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM (Freeze ResNet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
